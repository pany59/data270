{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "o3hAS_Fvj4f0"
   },
   "outputs": [],
   "source": [
    "# 可供参考 https://www.kaggle.com/code/surekharamireddy/spam-detection-with-99-accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Gr6Ht94-ItoE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 根目录\n",
    "root_dir = \"lingspam_public/bare\"\n",
    "\n",
    "# 创建DataFrame with 3 cols\n",
    "# df = pd.DataFrame(columns=[\"label\", \"subject\", \"main_body\"])\n",
    "df = []\n",
    "\n",
    "# 遍历根目录下的10个文件夹\n",
    "for i in range(1, 11):\n",
    "    folder = os.path.join(root_dir, \"part{}\".format(i))\n",
    "    file_list = os.listdir(folder)\n",
    "\n",
    "    # 遍历每个文件\n",
    "    for file_name in file_list:\n",
    "        # 重复文件名的去掉 df.drop_duplicates(subset=['file_name'], keep='first', inplace=True)\n",
    "\n",
    "        with open(os.path.join(folder, file_name), \"r\") as f:\n",
    "            file_content = f.readlines()\n",
    "\n",
    "            # 获取标签\n",
    "            if \"spm\" in file_name:\n",
    "                label = \"spam\"\n",
    "            else:\n",
    "                label = \"non-spam\"\n",
    "\n",
    "            # 获取主题行和正文\n",
    "            # 这里把Subject: 去掉了, 只留下subject本身\n",
    "            if file_name == \"Icon\": continue\n",
    "            if len(file_content) == 0:\n",
    "                print(folder, file_name)\n",
    "            else:\n",
    "                subject = file_content[0].replace(\"Subject: \", \"\").strip()\n",
    "            # 用strip()是为了防止存在换行符\\n\n",
    "            main_body = \"\".join([line.strip() for line in file_content[1:]])\n",
    "\n",
    "            # 将信息添加到DataFrame中\n",
    "            df.append(pd.DataFrame({\"label\": label, \"subject\": subject, \"main_body\": main_body}, index=[0]))\n",
    "\n",
    "# 将所有数据连接在一起\n",
    "df = pd.concat(df, ignore_index=True)\n",
    "\n",
    "# \"\"实际上不是空值, 他的值就是\"\"\n",
    "# 这里把所有\"\"替换成空值\n",
    "df[\"label\"].replace(\"\", np.nan, inplace=True)\n",
    "df[\"subject\"].replace(\"\", np.nan, inplace=True)\n",
    "df[\"main_body\"].replace(\"\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QMHq10LSItoG",
    "outputId": "cba645ce-527a-44b5-a0d7-c2cd284e063f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>subject</th>\n",
       "      <th>main_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>re : 2 . 882 s - &gt; np np</td>\n",
       "      <td>&gt; date : sun , 15 dec 91 02 : 25 : 02 est &gt; fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>s - &gt; np + np</td>\n",
       "      <td>the discussion of s - &gt; np + np reminds me tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>2 . 882 s - &gt; np np</td>\n",
       "      <td>. . . for me it 's much more restrictive than ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>gent conference</td>\n",
       "      <td>\" for the listserv \" international conference ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>query : causatives in korean</td>\n",
       "      <td>could anyone point me to any books and article...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2888</th>\n",
       "      <td>spam</td>\n",
       "      <td>lucky you !</td>\n",
       "      <td>congratulations ! you ' ve been selected to en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2889</th>\n",
       "      <td>spam</td>\n",
       "      <td>new on capitalfm . com</td>\n",
       "      <td>this is new at http : / / capitalfm . com - ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2890</th>\n",
       "      <td>spam</td>\n",
       "      <td>submit 600</td>\n",
       "      <td>this is not spam ; you are receiving this mess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2891</th>\n",
       "      <td>spam</td>\n",
       "      <td>submit 600</td>\n",
       "      <td>this is not spam ; you are receiving this mess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2892</th>\n",
       "      <td>spam</td>\n",
       "      <td>i can ' t stand it ! ! ! ! ! ! !</td>\n",
       "      <td>dear internet user : dear internet user : soun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2893 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                           subject  \\\n",
       "0     non-spam          re : 2 . 882 s - > np np   \n",
       "1     non-spam                     s - > np + np   \n",
       "2     non-spam               2 . 882 s - > np np   \n",
       "3     non-spam                   gent conference   \n",
       "4     non-spam      query : causatives in korean   \n",
       "...        ...                               ...   \n",
       "2888      spam                       lucky you !   \n",
       "2889      spam            new on capitalfm . com   \n",
       "2890      spam                        submit 600   \n",
       "2891      spam                        submit 600   \n",
       "2892      spam  i can ' t stand it ! ! ! ! ! ! !   \n",
       "\n",
       "                                              main_body  \n",
       "0     > date : sun , 15 dec 91 02 : 25 : 02 est > fr...  \n",
       "1     the discussion of s - > np + np reminds me tha...  \n",
       "2     . . . for me it 's much more restrictive than ...  \n",
       "3     \" for the listserv \" international conference ...  \n",
       "4     could anyone point me to any books and article...  \n",
       "...                                                 ...  \n",
       "2888  congratulations ! you ' ve been selected to en...  \n",
       "2889  this is new at http : / / capitalfm . com - ex...  \n",
       "2890  this is not spam ; you are receiving this mess...  \n",
       "2891  this is not spam ; you are receiving this mess...  \n",
       "2892  dear internet user : dear internet user : soun...  \n",
       "\n",
       "[2893 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7MgaaCRrj4f5"
   },
   "outputs": [],
   "source": [
    "# df.to_csv(\"dataframe.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QiIaXFhGj4f6"
   },
   "source": [
    "# assignment #7 参考kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "y-Logy_cj4f7",
    "outputId": "abe32cd0-0fc8-4246-ae3d-b2b3a714876b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label         0\n",
       "subject      62\n",
       "main_body     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Gd9hXKurj4f7",
    "outputId": "118db9a4-8ee1-4b1f-a639-68b9afcf4afd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>subject</th>\n",
       "      <th>main_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>re : 2 . 882 s - &gt; np np</td>\n",
       "      <td>&gt; date : sun , 15 dec 91 02 : 25 : 02 est &gt; fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>s - &gt; np + np</td>\n",
       "      <td>the discussion of s - &gt; np + np reminds me tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>2 . 882 s - &gt; np np</td>\n",
       "      <td>. . . for me it 's much more restrictive than ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>gent conference</td>\n",
       "      <td>\" for the listserv \" international conference ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>query : causatives in korean</td>\n",
       "      <td>could anyone point me to any books and article...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2888</th>\n",
       "      <td>spam</td>\n",
       "      <td>lucky you !</td>\n",
       "      <td>congratulations ! you ' ve been selected to en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2889</th>\n",
       "      <td>spam</td>\n",
       "      <td>new on capitalfm . com</td>\n",
       "      <td>this is new at http : / / capitalfm . com - ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2890</th>\n",
       "      <td>spam</td>\n",
       "      <td>submit 600</td>\n",
       "      <td>this is not spam ; you are receiving this mess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2891</th>\n",
       "      <td>spam</td>\n",
       "      <td>submit 600</td>\n",
       "      <td>this is not spam ; you are receiving this mess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2892</th>\n",
       "      <td>spam</td>\n",
       "      <td>i can ' t stand it ! ! ! ! ! ! !</td>\n",
       "      <td>dear internet user : dear internet user : soun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2893 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                           subject  \\\n",
       "0     non-spam          re : 2 . 882 s - > np np   \n",
       "1     non-spam                     s - > np + np   \n",
       "2     non-spam               2 . 882 s - > np np   \n",
       "3     non-spam                   gent conference   \n",
       "4     non-spam      query : causatives in korean   \n",
       "...        ...                               ...   \n",
       "2888      spam                       lucky you !   \n",
       "2889      spam            new on capitalfm . com   \n",
       "2890      spam                        submit 600   \n",
       "2891      spam                        submit 600   \n",
       "2892      spam  i can ' t stand it ! ! ! ! ! ! !   \n",
       "\n",
       "                                              main_body  \n",
       "0     > date : sun , 15 dec 91 02 : 25 : 02 est > fr...  \n",
       "1     the discussion of s - > np + np reminds me tha...  \n",
       "2     . . . for me it 's much more restrictive than ...  \n",
       "3     \" for the listserv \" international conference ...  \n",
       "4     could anyone point me to any books and article...  \n",
       "...                                                 ...  \n",
       "2888  congratulations ! you ' ve been selected to en...  \n",
       "2889  this is new at http : / / capitalfm . com - ex...  \n",
       "2890  this is not spam ; you are receiving this mess...  \n",
       "2891  this is not spam ; you are receiving this mess...  \n",
       "2892  dear internet user : dear internet user : soun...  \n",
       "\n",
       "[2893 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Data Cleaning: Handling of Incomplete & Missing Data\n",
    "\n",
    "miss label - drop\n",
    "miss subject - keep\n",
    "miss main body - drop\n",
    "'''\n",
    "# 如果主题行缺失，则用 Missing 替代\n",
    "# df[\"subject\"] = df[\"subject\"].fillna(\"missing\")\n",
    "df[\"subject\"].fillna(\"missing\", inplace=True)\n",
    "\n",
    "# 直接删掉\n",
    "df.dropna(subset=[\"label\"], inplace=True)\n",
    "df.dropna(subset=[\"main_body\"], inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wRz__hVWelFA",
    "outputId": "3c73bc96-fd3a-4987-b25b-f035776de56c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johnny\\AppData\\Local\\Temp\\ipykernel_26388\\2200824950.py:33: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['subject']=df['subject'].str.replace(r'\\d+(\\.\\d+)?', 'numbers')\n",
      "C:\\Users\\Johnny\\AppData\\Local\\Temp\\ipykernel_26388\\2200824950.py:34: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['main_body']=df['main_body'].str.replace(r'\\d+(\\.\\d+)?', 'numbers')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>subject</th>\n",
       "      <th>main_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>re  numbers  numbers s   np np</td>\n",
       "      <td>date  sun  numbers dec numbers numbers  numbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>s   np  np</td>\n",
       "      <td>the discussion of s   np  np reminds me that s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>numbers  numbers s   np np</td>\n",
       "      <td>for me it s much more restrictive than s   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>gent conference</td>\n",
       "      <td>for the listserv  international conference nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>query  causatives in korean</td>\n",
       "      <td>could anyone point me to any books and article...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2884</th>\n",
       "      <td>spam</td>\n",
       "      <td>angels  sent to serve mankind</td>\n",
       "      <td>learn to put angels to work  angels are anothe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2885</th>\n",
       "      <td>spam</td>\n",
       "      <td>lucky you</td>\n",
       "      <td>congratulations  you  ve been selected to ente...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2886</th>\n",
       "      <td>spam</td>\n",
       "      <td>review any book  pc or mac software pgm  consu...</td>\n",
       "      <td>we are celebrating our numbersth issue of our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>spam</td>\n",
       "      <td>stock market information for you</td>\n",
       "      <td>sender  trinity ventures  inc  address  number...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2892</th>\n",
       "      <td>spam</td>\n",
       "      <td>i can  t stand it</td>\n",
       "      <td>dear internet user  dear internet user  sound ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2597 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                            subject  \\\n",
       "0     non-spam                     re  numbers  numbers s   np np   \n",
       "1     non-spam                                         s   np  np   \n",
       "2     non-spam                         numbers  numbers s   np np   \n",
       "3     non-spam                                    gent conference   \n",
       "4     non-spam                        query  causatives in korean   \n",
       "...        ...                                                ...   \n",
       "2884      spam                      angels  sent to serve mankind   \n",
       "2885      spam                                         lucky you    \n",
       "2886      spam  review any book  pc or mac software pgm  consu...   \n",
       "2887      spam                   stock market information for you   \n",
       "2892      spam                           i can  t stand it          \n",
       "\n",
       "                                              main_body  \n",
       "0      date  sun  numbers dec numbers numbers  numbe...  \n",
       "1     the discussion of s   np  np reminds me that s...  \n",
       "2        for me it s much more restrictive than s   ...  \n",
       "3      for the listserv  international conference nu...  \n",
       "4     could anyone point me to any books and article...  \n",
       "...                                                 ...  \n",
       "2884  learn to put angels to work  angels are anothe...  \n",
       "2885  congratulations  you  ve been selected to ente...  \n",
       "2886  we are celebrating our numbersth issue of our ...  \n",
       "2887  sender  trinity ventures  inc  address  number...  \n",
       "2892  dear internet user  dear internet user  sound ...  \n",
       "\n",
       "[2597 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Data Cleaning: Handling of Noisy Data\n",
    "\n",
    "noisy data -> meaningless data: all punctuations -> drop\n",
    "noisy data -> redundant data: repetitive data -> drop\n",
    "# REPLACING EMAIL IDs BY 'MAILID'\n",
    "# REPLACING URLs  BY 'Links'\n",
    "# REPLACING CURRENCY SIGNS BY 'MONEY'\n",
    "# REPLACINg NUMBERS by 'numbers'\n",
    "'''\n",
    "\n",
    "# drop all punctuations\n",
    "df['subject'] = df['subject'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "df['main_body'] = df['main_body'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "# drop repetitive data\n",
    "df.drop_duplicates(subset=[\"subject\"], inplace=True) # 可以删掉\n",
    "df.drop_duplicates(subset=[\"main_body\"], inplace=True)\n",
    "\n",
    "# replace email by 'MailID'\n",
    "df['subject']=df['subject'].str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$','MailID', regex=True) # 可以删掉\n",
    "df['main_body']=df['main_body'].str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$','MailID', regex=True)\n",
    "\n",
    "# replace links by 'Links'\n",
    "df['subject']=df['subject'].str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$','Links', regex=True) # 可以删掉\n",
    "df['main_body']=df['main_body'].str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$','Links', regex=True)\n",
    "\n",
    "# replace currency by 'money'\n",
    "df['subject']=df['subject'].str.replace(r'£|\\$', 'money', regex=True)\n",
    "df['main_body']=df['main_body'].str.replace(r'£|\\$', 'money', regex=True)\n",
    "\n",
    "# replace numbers by 'numbers'\n",
    "df['subject']=df['subject'].str.replace(r'\\d+(\\.\\d+)?', 'numbers')\n",
    "df['main_body']=df['main_body'].str.replace(r'\\d+(\\.\\d+)?', 'numbers')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IPHepPBAelFB",
    "outputId": "c349f955-929b-4a01-b8c5-49813637b7a4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>subject</th>\n",
       "      <th>main_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>re numbers numbers s np np</td>\n",
       "      <td>date sun numbers dec numbers numbers numbers ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>s np np</td>\n",
       "      <td>the discussion of s np np reminds me that some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>numbers numbers s np np</td>\n",
       "      <td>for me it s much more restrictive than s np n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>gent conference</td>\n",
       "      <td>for the listserv international conference num...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>query causatives in korean</td>\n",
       "      <td>could anyone point me to any books and article...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2884</th>\n",
       "      <td>spam</td>\n",
       "      <td>angels sent to serve mankind</td>\n",
       "      <td>learn to put angels to work angels are another...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2885</th>\n",
       "      <td>spam</td>\n",
       "      <td>lucky you</td>\n",
       "      <td>congratulations you ve been selected to enter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2886</th>\n",
       "      <td>spam</td>\n",
       "      <td>review any book pc or mac software pgm consume...</td>\n",
       "      <td>we are celebrating our numbersth issue of our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>spam</td>\n",
       "      <td>stock market information for you</td>\n",
       "      <td>sender trinity ventures inc address numbers we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2892</th>\n",
       "      <td>spam</td>\n",
       "      <td>i can t stand it</td>\n",
       "      <td>dear internet user dear internet user sound fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2597 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                            subject  \\\n",
       "0     non-spam                         re numbers numbers s np np   \n",
       "1     non-spam                                            s np np   \n",
       "2     non-spam                            numbers numbers s np np   \n",
       "3     non-spam                                    gent conference   \n",
       "4     non-spam                         query causatives in korean   \n",
       "...        ...                                                ...   \n",
       "2884      spam                       angels sent to serve mankind   \n",
       "2885      spam                                         lucky you    \n",
       "2886      spam  review any book pc or mac software pgm consume...   \n",
       "2887      spam                   stock market information for you   \n",
       "2892      spam                                  i can t stand it    \n",
       "\n",
       "                                              main_body  \n",
       "0      date sun numbers dec numbers numbers numbers ...  \n",
       "1     the discussion of s np np reminds me that some...  \n",
       "2      for me it s much more restrictive than s np n...  \n",
       "3      for the listserv international conference num...  \n",
       "4     could anyone point me to any books and article...  \n",
       "...                                                 ...  \n",
       "2884  learn to put angels to work angels are another...  \n",
       "2885  congratulations you ve been selected to enter ...  \n",
       "2886  we are celebrating our numbersth issue of our ...  \n",
       "2887  sender trinity ventures inc address numbers we...  \n",
       "2892  dear internet user dear internet user sound fa...  \n",
       "\n",
       "[2597 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Data Cleaning: Handling of Inconsistent Data\n",
    "\n",
    "lowercase / uppercase -> all lowercase\n",
    "# REPLACING NEXT LINES BY 'WHITE SPACE'\n",
    "# REPLACING LARGE WHITE SPACE BY SINGLE WHITE SPACE\n",
    "# REPLACING LEADING AND TRAILING WHITE SPACE BY SINGLE WHITE SPACE\n",
    "# REPLACING SPECIAL CHARACTERS  BY WHITE SPACE\n",
    "'''\n",
    "\n",
    "# convert to lowercase\n",
    "df['subject']=df['subject'].str.lower()\n",
    "df['main_body']=df['main_body'].str.lower()\n",
    "\n",
    "# replace special characters by white space\n",
    "df['subject']=df['subject'].str.replace(r\"[^a-zA-Z0-9]+\", \" \", regex=True)\n",
    "df['main_body']=df['main_body'].str.replace(r\"[^a-zA-Z0-9]+\", \" \", regex=True)\n",
    "\n",
    "# replace leading and trailing white space by single white space\n",
    "df['subject']=df['subject'].str.replace(r'^\\s+|\\s+?$', ' ', regex=True)\n",
    "df['main_body']=df['main_body'].str.replace(r'^\\s+|\\s+?$', ' ', regex=True)\n",
    "\n",
    "# replace next line by white space\n",
    "df['subject']=df['subject'].str.replace(r'\\n',\" \", regex=True)\n",
    "df['main_body']=df['main_body'].str.replace(r'\\n',\" \", regex=True)\n",
    "\n",
    "# 这个要放最后\n",
    "# replace large white space by single white space\n",
    "df['subject']=df['subject'].str.replace(r'\\s+', ' ', regex=True)\n",
    "df['main_body']=df['main_body'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TKvHQ66mj4f9",
    "outputId": "6f86cc00-9768-4ea0-fc10-3953db85caa2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>subject</th>\n",
       "      <th>length</th>\n",
       "      <th>main_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>re numbers numbers s np np</td>\n",
       "      <td>947</td>\n",
       "      <td>date sun numbers dec numbers numbers numbers ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>s np np</td>\n",
       "      <td>394</td>\n",
       "      <td>the discussion of s np np reminds me that some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>numbers numbers s np np</td>\n",
       "      <td>93</td>\n",
       "      <td>for me it s much more restrictive than s np n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>gent conference</td>\n",
       "      <td>7118</td>\n",
       "      <td>for the listserv international conference num...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>non-spam</td>\n",
       "      <td>query causatives in korean</td>\n",
       "      <td>176</td>\n",
       "      <td>could anyone point me to any books and article...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2884</th>\n",
       "      <td>spam</td>\n",
       "      <td>angels sent to serve mankind</td>\n",
       "      <td>1176</td>\n",
       "      <td>learn to put angels to work angels are another...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2885</th>\n",
       "      <td>spam</td>\n",
       "      <td>lucky you</td>\n",
       "      <td>541</td>\n",
       "      <td>congratulations you ve been selected to enter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2886</th>\n",
       "      <td>spam</td>\n",
       "      <td>review any book pc or mac software pgm consume...</td>\n",
       "      <td>5837</td>\n",
       "      <td>we are celebrating our numbersth issue of our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>spam</td>\n",
       "      <td>stock market information for you</td>\n",
       "      <td>3918</td>\n",
       "      <td>sender trinity ventures inc address numbers we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2892</th>\n",
       "      <td>spam</td>\n",
       "      <td>i can t stand it</td>\n",
       "      <td>14111</td>\n",
       "      <td>dear internet user dear internet user sound fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2596 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                            subject  length  \\\n",
       "0     non-spam                         re numbers numbers s np np     947   \n",
       "1     non-spam                                            s np np     394   \n",
       "2     non-spam                            numbers numbers s np np      93   \n",
       "3     non-spam                                    gent conference    7118   \n",
       "4     non-spam                         query causatives in korean     176   \n",
       "...        ...                                                ...     ...   \n",
       "2884      spam                       angels sent to serve mankind    1176   \n",
       "2885      spam                                         lucky you      541   \n",
       "2886      spam  review any book pc or mac software pgm consume...    5837   \n",
       "2887      spam                   stock market information for you    3918   \n",
       "2892      spam                                  i can t stand it    14111   \n",
       "\n",
       "                                              main_body  \n",
       "0      date sun numbers dec numbers numbers numbers ...  \n",
       "1     the discussion of s np np reminds me that some...  \n",
       "2      for me it s much more restrictive than s np n...  \n",
       "3      for the listserv international conference num...  \n",
       "4     could anyone point me to any books and article...  \n",
       "...                                                 ...  \n",
       "2884  learn to put angels to work angels are another...  \n",
       "2885  congratulations you ve been selected to enter ...  \n",
       "2886  we are celebrating our numbersth issue of our ...  \n",
       "2887  sender trinity ventures inc address numbers we...  \n",
       "2892  dear internet user dear internet user sound fa...  \n",
       "\n",
       "[2596 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在完成后续的操作以后可能原本不是空的main body会变成空\n",
    "# 所以可能需要再次处理\n",
    "df[\"subject\"].replace(\" \", np.nan, inplace=True)\n",
    "df[\"main_body\"].replace(\" \", np.nan, inplace=True)\n",
    "\n",
    "df[\"subject\"].fillna(\"missing\", inplace=True)\n",
    "df.dropna(subset=[\"main_body\"], inplace=True)\n",
    "\n",
    "df.insert(len(df.columns)-1, 'length', df['main_body'].apply(len))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "aMQY3sQlj4f-"
   },
   "outputs": [],
   "source": [
    "# df.to_csv(\"cleaned_data.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dPyKfsZfkiGF"
   },
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5yuJ-Ay9kP8T"
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TTWbRe4uelFD"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# removing stopwords\n",
    "stop = stopwords.words('english')\n",
    "df['subject'] = df['subject'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df['main_body'] = df['main_body'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "# Apply lemmatization\n",
    "lemm = WordNetLemmatizer()\n",
    "df['subject'] = df['subject'].apply(lambda x: ' '.join([lemm.lemmatize(word, pos=\"v\") for word in x.split()]))\n",
    "df['main_body'] = df['main_body'].apply(lambda x: ' '.join([lemm.lemmatize(word, pos=\"v\") for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7VDnfJHNb3Vk"
   },
   "outputs": [],
   "source": [
    "df[\"label\"].replace(\"non-spam\", 0, inplace=True)\n",
    "df[\"label\"].replace(\"spam\", 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "UiCSpfPrlWLH",
    "outputId": "0fe333ac-0abd-4a18-f881-df830758f3d7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>subject</th>\n",
       "      <th>length</th>\n",
       "      <th>main_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>number number np np</td>\n",
       "      <td>626</td>\n",
       "      <td>date sun number dec number number number numbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>np np</td>\n",
       "      <td>295</td>\n",
       "      <td>discussion np np remind years ago read source ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>number number np np</td>\n",
       "      <td>51</td>\n",
       "      <td>much restrictive np np np pro quite overrestri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>gent conference</td>\n",
       "      <td>5491</td>\n",
       "      <td>listserv international conference number secon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>query causatives korean</td>\n",
       "      <td>137</td>\n",
       "      <td>could anyone point book article causative cons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2884</th>\n",
       "      <td>1</td>\n",
       "      <td>angels send serve mankind</td>\n",
       "      <td>750</td>\n",
       "      <td>learn put angels work angels another race be o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2885</th>\n",
       "      <td>1</td>\n",
       "      <td>lucky</td>\n",
       "      <td>352</td>\n",
       "      <td>congratulations select enter vacation adventur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2886</th>\n",
       "      <td>1</td>\n",
       "      <td>review book pc mac software pgm consumer produ...</td>\n",
       "      <td>4237</td>\n",
       "      <td>celebrate numbersth issue inet review newslett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>1</td>\n",
       "      <td>stock market information</td>\n",
       "      <td>2959</td>\n",
       "      <td>sender trinity venture inc address number west...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2892</th>\n",
       "      <td>1</td>\n",
       "      <td>stand</td>\n",
       "      <td>9835</td>\n",
       "      <td>dear internet user dear internet user sound fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2596 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                            subject  length  \\\n",
       "0         0                                number number np np     626   \n",
       "1         0                                              np np     295   \n",
       "2         0                                number number np np      51   \n",
       "3         0                                    gent conference    5491   \n",
       "4         0                            query causatives korean     137   \n",
       "...     ...                                                ...     ...   \n",
       "2884      1                          angels send serve mankind     750   \n",
       "2885      1                                              lucky     352   \n",
       "2886      1  review book pc mac software pgm consumer produ...    4237   \n",
       "2887      1                           stock market information    2959   \n",
       "2892      1                                              stand    9835   \n",
       "\n",
       "                                              main_body  \n",
       "0     date sun number dec number number number numbe...  \n",
       "1     discussion np np remind years ago read source ...  \n",
       "2     much restrictive np np np pro quite overrestri...  \n",
       "3     listserv international conference number secon...  \n",
       "4     could anyone point book article causative cons...  \n",
       "...                                                 ...  \n",
       "2884  learn put angels work angels another race be o...  \n",
       "2885  congratulations select enter vacation adventur...  \n",
       "2886  celebrate numbersth issue inet review newslett...  \n",
       "2887  sender trinity venture inc address number west...  \n",
       "2892  dear internet user dear internet user sound fa...  \n",
       "\n",
       "[2596 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see the change in text length after removing stop words\n",
    "df['length']=df['main_body'].apply(len)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "I-_K1rsselFE"
   },
   "outputs": [],
   "source": [
    "# 更正由于drop stop words导致的空值\n",
    "df['subject']=df['subject'].str.replace(r'\\s+', ' ', regex=True)\n",
    "df['main_body']=df['main_body'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "df[\"subject\"].replace(\" \", np.nan, inplace=True)\n",
    "df[\"main_body\"].replace(\" \", np.nan, inplace=True)\n",
    "\n",
    "df[\"subject\"].fillna(\"missing\", inplace=True)\n",
    "df.dropna(subset=[\"main_body\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "SAWeOR-UelFE"
   },
   "outputs": [],
   "source": [
    "# df.to_csv(\"removedstop_cleaned_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "kekL7p5Flc7A",
    "outputId": "bd441dfd-e999-440c-c9df-b6c42d239afc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaai</th>\n",
       "      <th>aaainumbers</th>\n",
       "      <th>aaal</th>\n",
       "      <th>aaanumbers</th>\n",
       "      <th>aaarghh</th>\n",
       "      <th>aaas</th>\n",
       "      <th>aabb</th>\n",
       "      <th>aabyhoej</th>\n",
       "      <th>...</th>\n",
       "      <th>zwischen</th>\n",
       "      <th>zwitserlood</th>\n",
       "      <th>zxgahnumbersqabjh</th>\n",
       "      <th>zybatov</th>\n",
       "      <th>zybatow</th>\n",
       "      <th>zygmunt</th>\n",
       "      <th>zyokyoozyu</th>\n",
       "      <th>zytkow</th>\n",
       "      <th>zzlsa</th>\n",
       "      <th>zznumbers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2596 rows × 51266 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aa  aaa  aaai  aaainumbers  aaal  aaanumbers  aaarghh  aaas  aabb  \\\n",
       "0     0.0  0.0   0.0          0.0   0.0         0.0      0.0   0.0   0.0   \n",
       "1     0.0  0.0   0.0          0.0   0.0         0.0      0.0   0.0   0.0   \n",
       "2     0.0  0.0   0.0          0.0   0.0         0.0      0.0   0.0   0.0   \n",
       "3     0.0  0.0   0.0          0.0   0.0         0.0      0.0   0.0   0.0   \n",
       "4     0.0  0.0   0.0          0.0   0.0         0.0      0.0   0.0   0.0   \n",
       "...   ...  ...   ...          ...   ...         ...      ...   ...   ...   \n",
       "2591  0.0  0.0   0.0          0.0   0.0         0.0      0.0   0.0   0.0   \n",
       "2592  0.0  0.0   0.0          0.0   0.0         0.0      0.0   0.0   0.0   \n",
       "2593  0.0  0.0   0.0          0.0   0.0         0.0      0.0   0.0   0.0   \n",
       "2594  0.0  0.0   0.0          0.0   0.0         0.0      0.0   0.0   0.0   \n",
       "2595  0.0  0.0   0.0          0.0   0.0         0.0      0.0   0.0   0.0   \n",
       "\n",
       "      aabyhoej  ...  zwischen  zwitserlood  zxgahnumbersqabjh  zybatov  \\\n",
       "0          0.0  ...       0.0          0.0                0.0      0.0   \n",
       "1          0.0  ...       0.0          0.0                0.0      0.0   \n",
       "2          0.0  ...       0.0          0.0                0.0      0.0   \n",
       "3          0.0  ...       0.0          0.0                0.0      0.0   \n",
       "4          0.0  ...       0.0          0.0                0.0      0.0   \n",
       "...        ...  ...       ...          ...                ...      ...   \n",
       "2591       0.0  ...       0.0          0.0                0.0      0.0   \n",
       "2592       0.0  ...       0.0          0.0                0.0      0.0   \n",
       "2593       0.0  ...       0.0          0.0                0.0      0.0   \n",
       "2594       0.0  ...       0.0          0.0                0.0      0.0   \n",
       "2595       0.0  ...       0.0          0.0                0.0      0.0   \n",
       "\n",
       "      zybatow  zygmunt  zyokyoozyu  zytkow  zzlsa  zznumbers  \n",
       "0         0.0      0.0         0.0     0.0    0.0        0.0  \n",
       "1         0.0      0.0         0.0     0.0    0.0        0.0  \n",
       "2         0.0      0.0         0.0     0.0    0.0        0.0  \n",
       "3         0.0      0.0         0.0     0.0    0.0        0.0  \n",
       "4         0.0      0.0         0.0     0.0    0.0        0.0  \n",
       "...       ...      ...         ...     ...    ...        ...  \n",
       "2591      0.0      0.0         0.0     0.0    0.0        0.0  \n",
       "2592      0.0      0.0         0.0     0.0    0.0        0.0  \n",
       "2593      0.0      0.0         0.0     0.0    0.0        0.0  \n",
       "2594      0.0      0.0         0.0     0.0    0.0        0.0  \n",
       "2595      0.0      0.0         0.0     0.0    0.0        0.0  \n",
       "\n",
       "[2596 rows x 51266 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(df['main_body'])\n",
    "df1 = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ywwUAB9uliXg",
    "outputId": "0aa6b66c-4b5b-4731-f902-3cd95b34452d"
   },
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# # n_components must be between 1 and min(vectors.shape)\n",
    "# svd = TruncatedSVD(n_components=1750)\n",
    "# svd.fit(vectors)\n",
    "# print(svd.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "D4bbVs7qlllW",
    "outputId": "5c2474e7-eb55-4fe8-8c08-0506cd28ad11"
   },
   "outputs": [],
   "source": [
    "# transformed_vectors = svd.transform(vectors)\n",
    "# print(transformed_vectors)\n",
    "# print(\"dimension =\", transformed_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "xGGfvbQab3Vn",
    "outputId": "c0bf1fb9-4596-458f-e66b-ce3b7d8b45e5"
   },
   "outputs": [],
   "source": [
    "# 降维后的\n",
    "# df1 = pd.DataFrame(transformed_vectors)\n",
    "# df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "P71sJvnwPxNl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\\nfrom transformers import BertModel,BertTokenizer\\nimport torch\\n\\nbert_model = BertModel.from_pretrained(\\'bert-base-uncased\\', output_hidden_states = True)\\nbert_tokenizer = BertTokenizer.from_pretrained(\\'bert-base-uncased\\')\\n\\ndef get_bert_embeddings(text, model, tokenizer):\\n\\n    marked_text = \"[CLS] \" + text + \" [SEP]\"\\n    tokenized_text = tokenizer.tokenize(marked_text)\\n    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\\n    segments_ids = [1]*len(indexed_tokens)\\n\\n    # convert inputs to tensors\\n    tokens_tensor = torch.tensor([indexed_tokens])\\n    segments_tensor = torch.tensor([segments_ids])\\n\\n    with torch.no_grad():\\n        # obtain hidden states\\n        outputs = model(tokens_tensor, segments_tensor)\\n        hidden_states = outputs[2]\\n\\n    # `token_vecs` is a tensor with shape [22 x 768]\\n    token_vecs = hidden_states[-2][0]\\n\\n    # Calculate the average of all 22 token vectors.\\n    sentence_embedding = torch.mean(token_vecs, dim=0)\\n\\n    return sentence_embedding'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "from transformers import BertModel,BertTokenizer\n",
    "import torch\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embeddings(text, model, tokenizer):\n",
    "\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "    # convert inputs to tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = torch.tensor([segments_ids])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # obtain hidden states\n",
    "        outputs = model(tokens_tensor, segments_tensor)\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # `token_vecs` is a tensor with shape [22 x 768]\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "\n",
    "    # Calculate the average of all 22 token vectors.\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "    return sentence_embedding'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Ci_sCozO4xuu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_bert_word_vectors(text, model, tokenizer):\\n\\n    marked_text = \"[CLS] \" + text + \" [SEP]\"\\n    tokenized_text = tokenizer.tokenize(marked_text)\\n    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\\n    segments_ids = [1]*len(indexed_tokens)\\n\\n    # convert inputs to tensors\\n    tokens_tensor = torch.tensor([indexed_tokens])\\n    segments_tensor = torch.tensor([segments_ids])\\n    \\n    with torch.no_grad():\\n        # obtain hidden states\\n        outputs = model(tokens_tensor, segments_tensor)\\n        hidden_states = outputs[2]\\n\\n    # concatenate the tensors for all layers\\n    # use \"stack\" to create new dimension in tensor\\n    token_embeddings = torch.stack(hidden_states, dim=0)\\n\\n    # # remove dimension 1, the \"batches\"\\n    token_embeddings = torch.squeeze(token_embeddings, dim=1)\\n\\n    # # swap dimensions 0 and 1 so we can loop over tokens\\n    token_embeddings = token_embeddings.permute(1,0,2)\\n\\n    # # intialized list to store embeddings\\n    token_vecs_sum = []\\n\\n    # # \"token_embeddings\" is a [Y x 12 x 768] tensor\\n    # # where Y is the number of tokens in the sentence\\n\\n    # # loop over tokens in sentence\\n    for token in token_embeddings:\\n\\n        # \"token\" is a [12 x 768] tensor\\n\\n        # sum the vectors from the last four layers\\n        sum_vec = torch.sum(token[-4:], dim=0)\\n        token_vecs_sum.append(sum_vec)\\n\\n    return token_vecs_sum\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果需要做word vectors的话\n",
    "\"\"\"\n",
    "def get_bert_word_vectors(text, model, tokenizer):\n",
    "\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "    # convert inputs to tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = torch.tensor([segments_ids])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # obtain hidden states\n",
    "        outputs = model(tokens_tensor, segments_tensor)\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # concatenate the tensors for all layers\n",
    "    # use \"stack\" to create new dimension in tensor\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    # # remove dimension 1, the \"batches\"\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # # swap dimensions 0 and 1 so we can loop over tokens\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    # # intialized list to store embeddings\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # # \"token_embeddings\" is a [Y x 12 x 768] tensor\n",
    "    # # where Y is the number of tokens in the sentence\n",
    "\n",
    "    # # loop over tokens in sentence\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # \"token\" is a [12 x 768] tensor\n",
    "\n",
    "        # sum the vectors from the last four layers\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "\n",
    "    return token_vecs_sum\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "HhmbW-e-ROqO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Bert Embedding 前510字 （能运行）\\ndf['truncated'] = df['main_body'].apply(lambda x: x[:510] if len(x)>510 else x)\\ndf['bert_embedding'] = df['truncated'].apply(lambda x: get_bert_embeddings(x, bert_model, bert_tokenizer))\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Bert Embedding 前510字 （能运行）\n",
    "df['truncated'] = df['main_body'].apply(lambda x: x[:510] if len(x)>510 else x)\n",
    "df['bert_embedding'] = df['truncated'].apply(lambda x: get_bert_embeddings(x, bert_model, bert_tokenizer))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "6D8nvab559NB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import LongformerTokenizer, EncoderDecoderModel\\n\\n# Load model and tokenizer\\nlong_model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/longformer2roberta-cnn_dailymail-fp16\")\\nlong_tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\") \\n\\ndef get_summary(text, model, tokenizer):\\n\\n    # Tokenize and summarize\\n    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\\n    output_ids = model.generate(input_ids)\\n\\n    # Get the summary from the output tokens\\n    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\\n    \\n    return summary\\n\\ndf[\\'summary\\'] = df[\\'main_body\\'].apply(lambda x: get_summary(x, long_model, long_tokenizer) if len(x)>510 else x)   \\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 网上找的几种方法 但都经常报错（运行整个dataframe时）\n",
    "\n",
    "# 1 https://huggingface.co/facebook/bart-large-cnn\n",
    "\"\"\"\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "text = df['main_body'][3]\n",
    "summary = summarizer(text, max_length=510, min_length=50, do_sample=False)[0]['summary_text']\n",
    "print(summary)\n",
    "print(len(summary))\n",
    "\n",
    "df['summary'] = df['main_body'].apply(lambda x: summarizer(x, max_length=510, min_length=50, do_sample=False)[0]['summary_text'] if len(x)>510 else x)\n",
    "\"\"\"\n",
    "\n",
    "# 2 https://github.com/christianversloot/machine-learning-articles/blob/main/transformers-for-long-text-code-examples-with-longformer.md\n",
    "\"\"\"\n",
    "from transformers import LongformerTokenizer, EncoderDecoderModel\n",
    "\n",
    "# Load model and tokenizer\n",
    "long_model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/longformer2roberta-cnn_dailymail-fp16\")\n",
    "long_tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\") \n",
    "\n",
    "def get_summary(text, model, tokenizer):\n",
    "\n",
    "    # Tokenize and summarize\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "    output_ids = model.generate(input_ids)\n",
    "\n",
    "    # Get the summary from the output tokens\n",
    "    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "df['summary'] = df['main_body'].apply(lambda x: get_summary(x, long_model, long_tokenizer) if len(x)>510 else x)   \n",
    "\"\"\"\n",
    "\n",
    "# https://towardsdatascience.com/text-summarization-with-nlp-textrank-vs-seq2seq-vs-bart-474943efeb09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ei19T9aHAOQj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport gensim\\nfrom nltk.tokenize import word_tokenize\\n\\n# Tokenize the text in the column\\ndf['tokenized_text'] = df['main_body'].apply(lambda x: word_tokenize(x))\\n\\nmodel = gensim.models.Word2Vec(df['tokenized_text'], vector_size=5000, window=5, min_count=1, workers=4)\\n\\ndf['embedding_vectors'] = df['tokenized_text'].apply(lambda x: [model.wv[word] for word in x])\\n\\ndf3 = pd.DataFrame(df['embedding_vectors'].tolist())\\ndf3\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 试了word2vec 但df3里面的vector维度都不一样\n",
    "# 也试了下doc2vec 也不行\n",
    "# 有兴趣可以看下 https://towardsdatascience.com/how-to-vectorize-text-in-dataframes-for-nlp-tasks-3-simple-techniques-82925a5600db\n",
    "\"\"\"\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize the text in the column\n",
    "df['tokenized_text'] = df['main_body'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "model = gensim.models.Word2Vec(df['tokenized_text'], vector_size=5000, window=5, min_count=1, workers=4)\n",
    "\n",
    "df['embedding_vectors'] = df['tokenized_text'].apply(lambda x: [model.wv[word] for word in x])\n",
    "\n",
    "df3 = pd.DataFrame(df['embedding_vectors'].tolist())\n",
    "df3\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AANSjbcK-HU3"
   },
   "source": [
    "Bag of Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "X7AfCnvN-FCk",
    "outputId": "b787be9e-0a8d-4eb8-f0b6-99cb4431f439"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in transformed df:  51284\n"
     ]
    }
   ],
   "source": [
    "# Count how many times a word appears in the dataset\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "total_counts = Counter()\n",
    "for i in range(len(df['main_body'])):\n",
    "    for word in df['main_body'].values[i].split(\" \"):\n",
    "        total_counts[word] += 1\n",
    "\n",
    "print(\"Total words in transformed df: \", len(total_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "qvEDc-oi-FUP",
    "outputId": "fefb4c16-5e6e-4589-c590-3d6f9380fa19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 words:  \n",
      " ['number', 'university', 'language', 'paper', 'email', 'information', 'linguistics', 'address', 'use', 'de', 'one', 'conference', 'send', 'e', 'order', 'please', 'languages', 'make', 'work', 'english', 'include', 'mail', 'http', 'program', 'also', 'edu', 'would', 'new', 'name', 'may', 'fax']\n"
     ]
    }
   ],
   "source": [
    "# Sort in decreasing order (Word with highest frequency appears first)\n",
    "vocab = sorted(total_counts, key=total_counts.get, reverse=True)\n",
    "print('Top 30 words: ', '\\n', vocab[:31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "xTjS-3w0-Fas"
   },
   "outputs": [],
   "source": [
    "# Map words to index\n",
    "vocab_size = len(vocab)\n",
    "word2idx = {}\n",
    "\n",
    "# print vocab_size\n",
    "for i, word in enumerate(vocab):\n",
    "    word2idx[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "uPgtnJsA-Fe2",
    "outputId": "a098c3fa-5f84-43c2-a3eb-722f1e1c8c89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2596, 51284)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to convert text to vectors\n",
    "def text_to_vector(text, vocab):\n",
    "    vector = np.zeros(len(vocab), dtype=np.int_)\n",
    "    for word in text.split():\n",
    "        if word in vocab:\n",
    "            index = vocab.index(word)\n",
    "            vector[index] += 1\n",
    "    return vector\n",
    "\n",
    "# Convert all text to vectors\n",
    "word_vectors = np.zeros((len(df['main_body']), len(vocab)), dtype=np.int_)\n",
    "\n",
    "for i, text in enumerate(df['main_body']):\n",
    "    word_vectors[i] = text_to_vector(text, vocab)\n",
    "    \n",
    "word_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "B-Q9ZzwR-Fkl",
    "outputId": "ac08df1f-794f-4652-9b61-85ecc9006f8c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>university</th>\n",
       "      <th>language</th>\n",
       "      <th>paper</th>\n",
       "      <th>email</th>\n",
       "      <th>information</th>\n",
       "      <th>linguistics</th>\n",
       "      <th>address</th>\n",
       "      <th>use</th>\n",
       "      <th>de</th>\n",
       "      <th>...</th>\n",
       "      <th>lifet</th>\n",
       "      <th>erform</th>\n",
       "      <th>promoti</th>\n",
       "      <th>onal</th>\n",
       "      <th>tow</th>\n",
       "      <th>urchase</th>\n",
       "      <th>crespo</th>\n",
       "      <th>tvsrnumbers</th>\n",
       "      <th>promotio</th>\n",
       "      <th>uarantee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2596 rows × 51284 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      number  university  language  paper  email  information  linguistics  \\\n",
       "0          7           0         1      0      0            0            0   \n",
       "1          1           1         0      0      0            0            0   \n",
       "2          0           0         0      0      0            0            0   \n",
       "3         89           6         4      2      1            0            1   \n",
       "4          0           0         0      0      1            0            0   \n",
       "...      ...         ...       ...    ...    ...          ...          ...   \n",
       "2591       6           0         0      1      3            0            0   \n",
       "2592       1           0         0      0      0            0            0   \n",
       "2593      22           0         0      0     10            3            0   \n",
       "2594      26           0         0      0      3            3            0   \n",
       "2595     254           0         0      0      3            3            0   \n",
       "\n",
       "      address  use  de  ...  lifet  erform  promoti  onal  tow  urchase  \\\n",
       "0           0    0   0  ...      0       0        0     0    0        0   \n",
       "1           0    0   0  ...      0       0        0     0    0        0   \n",
       "2           0    0   0  ...      0       0        0     0    0        0   \n",
       "3           1    1   0  ...      0       0        0     0    0        0   \n",
       "4           0    0   0  ...      0       0        0     0    0        0   \n",
       "...       ...  ...  ..  ...    ...     ...      ...   ...  ...      ...   \n",
       "2591        2    0   0  ...      0       0        0     0    0        0   \n",
       "2592        0    0   0  ...      0       0        0     0    0        0   \n",
       "2593        3    4   0  ...      0       0        0     0    0        0   \n",
       "2594        4    0   0  ...      0       0        0     0    0        0   \n",
       "2595        2    1   1  ...      1       1        1     1    1        1   \n",
       "\n",
       "      crespo  tvsrnumbers  promotio  uarantee  \n",
       "0          0            0         0         0  \n",
       "1          0            0         0         0  \n",
       "2          0            0         0         0  \n",
       "3          0            0         0         0  \n",
       "4          0            0         0         0  \n",
       "...      ...          ...       ...       ...  \n",
       "2591       0            0         0         0  \n",
       "2592       0            0         0         0  \n",
       "2593       0            0         0         0  \n",
       "2594       0            0         0         0  \n",
       "2595       1            1         1         1  \n",
       "\n",
       "[2596 rows x 51284 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert transformed vectors to dataframe to visualize\n",
    "df2 = pd.DataFrame(word_vectors, columns=vocab)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "jDkG6ypU-Fpi",
    "outputId": "581e5f7a-15a3-4a3a-cdfa-df97947d40aa"
   },
   "outputs": [],
   "source": [
    "# apply SVD\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# # n_components must be between 1 and min(vectors.shape)\n",
    "# svd = TruncatedSVD(n_components=500)\n",
    "# svd.fit(word_vectors)\n",
    "# print(svd.explained_variance_ratio_.sum()) #95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "CMavljEA-Xsk",
    "outputId": "066bd8b8-3cd6-4e6c-c65b-a91b1d6e8670"
   },
   "outputs": [],
   "source": [
    "# bow_transformed_vectors = svd.transform(word_vectors)\n",
    "# print(bow_transformed_vectors)\n",
    "# print(\"dimension =\", bow_transformed_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "xyExeVldb3Vr",
    "outputId": "730b4e46-d84b-463b-c2e7-0e78547bd092"
   },
   "outputs": [],
   "source": [
    "# 降维后的\n",
    "# df2 = pd.DataFrame(transformed_vectors)\n",
    "# df2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDIDF - NB\n",
    "# Naive Bayes does not work with SVD or other matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaai</th>\n",
       "      <th>aaainumbers</th>\n",
       "      <th>aaal</th>\n",
       "      <th>aaanumbers</th>\n",
       "      <th>aaarghh</th>\n",
       "      <th>aaas</th>\n",
       "      <th>aabb</th>\n",
       "      <th>aabyhoej</th>\n",
       "      <th>...</th>\n",
       "      <th>zwischen</th>\n",
       "      <th>zwitserlood</th>\n",
       "      <th>zxgahnumbersqabjh</th>\n",
       "      <th>zybatov</th>\n",
       "      <th>zybatow</th>\n",
       "      <th>zygmunt</th>\n",
       "      <th>zyokyoozyu</th>\n",
       "      <th>zytkow</th>\n",
       "      <th>zzlsa</th>\n",
       "      <th>zznumbers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2596 rows × 51266 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aa  aaa  aaai  aaainumbers  aaal  aaanumbers  aaarghh  aaas  aabb  \\\n",
       "0     0.0  0.0   0.0          0.0   0.0         0.0      0.0   0.0   0.0   \n",
       "1     0.0  0.0   0.0          0.0   0.0         0.0      0.0   0.0   0.0   \n",
       "2     0.0  0.0   0.0          0.0   0.0         0.0      0.0   0.0   0.0   \n",
       "3     0.0  0.0   0.0          0.0   0.0         0.0      0.0   0.0   0.0   \n",
       "4     0.0  0.0   0.0          0.0   0.0         0.0      0.0   0.0   0.0   \n",
       "...   ...  ...   ...          ...   ...         ...      ...   ...   ...   \n",
       "2591  0.0  0.0   0.0          0.0   0.0         0.0      0.0   0.0   0.0   \n",
       "2592  0.0  0.0   0.0          0.0   0.0         0.0      0.0   0.0   0.0   \n",
       "2593  0.0  0.0   0.0          0.0   0.0         0.0      0.0   0.0   0.0   \n",
       "2594  0.0  0.0   0.0          0.0   0.0         0.0      0.0   0.0   0.0   \n",
       "2595  0.0  0.0   0.0          0.0   0.0         0.0      0.0   0.0   0.0   \n",
       "\n",
       "      aabyhoej  ...  zwischen  zwitserlood  zxgahnumbersqabjh  zybatov  \\\n",
       "0          0.0  ...       0.0          0.0                0.0      0.0   \n",
       "1          0.0  ...       0.0          0.0                0.0      0.0   \n",
       "2          0.0  ...       0.0          0.0                0.0      0.0   \n",
       "3          0.0  ...       0.0          0.0                0.0      0.0   \n",
       "4          0.0  ...       0.0          0.0                0.0      0.0   \n",
       "...        ...  ...       ...          ...                ...      ...   \n",
       "2591       0.0  ...       0.0          0.0                0.0      0.0   \n",
       "2592       0.0  ...       0.0          0.0                0.0      0.0   \n",
       "2593       0.0  ...       0.0          0.0                0.0      0.0   \n",
       "2594       0.0  ...       0.0          0.0                0.0      0.0   \n",
       "2595       0.0  ...       0.0          0.0                0.0      0.0   \n",
       "\n",
       "      zybatow  zygmunt  zyokyoozyu  zytkow  zzlsa  zznumbers  \n",
       "0         0.0      0.0         0.0     0.0    0.0        0.0  \n",
       "1         0.0      0.0         0.0     0.0    0.0        0.0  \n",
       "2         0.0      0.0         0.0     0.0    0.0        0.0  \n",
       "3         0.0      0.0         0.0     0.0    0.0        0.0  \n",
       "4         0.0      0.0         0.0     0.0    0.0        0.0  \n",
       "...       ...      ...         ...     ...    ...        ...  \n",
       "2591      0.0      0.0         0.0     0.0    0.0        0.0  \n",
       "2592      0.0      0.0         0.0     0.0    0.0        0.0  \n",
       "2593      0.0      0.0         0.0     0.0    0.0        0.0  \n",
       "2594      0.0      0.0         0.0     0.0    0.0        0.0  \n",
       "2595      0.0      0.0         0.0     0.0    0.0        0.0  \n",
       "\n",
       "[2596 rows x 51266 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(df['main_body'])\n",
    "df1 = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_X_tfidf, test_X_tfidf, train_val_y_tfidf, test_y_tfidf = train_test_split(df1, df['label'], test_size=0.2, random_state=42)\n",
    "train_X_tfidf, val_X_tfidf, train_y_tfidf, val_y_tfidf = train_test_split(train_val_X_tfidf, train_val_y_tfidf, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "KAZ1akc2b3Vv"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[442  74]\n",
      " [  0   3]]\n",
      "Accuracy:  0.8574181117533719\n",
      "F1 score:  0.9178554189243003\n"
     ]
    }
   ],
   "source": [
    "mnb_1 = MultinomialNB(alpha=1.0) # the default value of alpha\n",
    "mnb_1.fit(train_X_tfidf,train_y_tfidf)\n",
    "\n",
    "# Evaluate the performance of the model on the validation set\n",
    "y_pred = mnb_1.predict(val_X_tfidf)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_pred,val_y_tfidf))\n",
    "print(\"Accuracy: \", accuracy_score(y_pred,val_y_tfidf))\n",
    "print(\"F1 score: \", f1_score(y_pred,val_y_tfidf, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[442  67]\n",
      " [  0  10]]\n",
      "Accuracy:  0.8709055876685935\n",
      "F1 score:  0.9160668658212969\n"
     ]
    }
   ],
   "source": [
    "mnb_05 = MultinomialNB(alpha=0.5)\n",
    "mnb_05.fit(train_X_tfidf,train_y_tfidf)\n",
    "\n",
    "# Evaluate the performance of the model on the validation set\n",
    "y_pred = mnb_05.predict(val_X_tfidf)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_pred,val_y_tfidf))\n",
    "print(\"Accuracy: \", accuracy_score(y_pred,val_y_tfidf))\n",
    "print(\"F1 score: \", f1_score(y_pred,val_y_tfidf, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[442  33]\n",
      " [  0  44]]\n",
      "Accuracy:  0.9364161849710982\n",
      "F1 score:  0.9439426125654781\n"
     ]
    }
   ],
   "source": [
    "mnb_025 = MultinomialNB(alpha=0.25)\n",
    "mnb_025.fit(train_X_tfidf,train_y_tfidf)\n",
    "\n",
    "# Evaluate the performance of the model on the validation set\n",
    "y_pred = mnb_025.predict(val_X_tfidf)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_pred,val_y_tfidf))\n",
    "print(\"Accuracy: \", accuracy_score(y_pred,val_y_tfidf))\n",
    "print(\"F1 score: \", f1_score(y_pred,val_y_tfidf, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[442   8]\n",
      " [  0  69]]\n",
      "Accuracy:  0.9845857418111753\n",
      "F1 score:  0.9849389280206742\n"
     ]
    }
   ],
   "source": [
    "mnb_01 = MultinomialNB(alpha=0.1)\n",
    "mnb_01.fit(train_X_tfidf,train_y_tfidf)\n",
    "\n",
    "# Evaluate the performance of the model on the validation set\n",
    "y_pred = mnb_01.predict(val_X_tfidf)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_pred,val_y_tfidf))\n",
    "print(\"Accuracy: \", accuracy_score(y_pred,val_y_tfidf))\n",
    "print(\"F1 score: \", f1_score(y_pred,val_y_tfidf, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[441   1]\n",
      " [  1  76]]\n",
      "Accuracy:  0.9961464354527938\n",
      "F1 score:  0.9961464354527938\n"
     ]
    }
   ],
   "source": [
    "mnb_001 = MultinomialNB(alpha=0.01)\n",
    "mnb_001.fit(train_X_tfidf,train_y_tfidf)\n",
    "\n",
    "# Evaluate the performance of the model on the validation set\n",
    "y_pred = mnb_001.predict(val_X_tfidf)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_pred,val_y_tfidf))\n",
    "print(\"Accuracy: \", accuracy_score(y_pred,val_y_tfidf))\n",
    "print(\"F1 score: \", f1_score(y_pred,val_y_tfidf, average = 'weighted'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the optimal alpha for tfidf vectors is 0.01. Now we can evaluate the final model with the alpha = 0.01 on the test set to obtain an unbiased estimate of its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[422   1]\n",
      " [  7  90]]\n",
      "Accuracy:  0.9846153846153847\n",
      "F1 score:  0.9844240566146468\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the model on the test set\n",
    "y_pred = mnb_001.predict(test_X_tfidf)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_pred,test_y_tfidf))\n",
    "print(\"Accuracy: \", accuracy_score(y_pred,test_y_tfidf))\n",
    "print(\"F1 score: \", f1_score(y_pred,test_y_tfidf, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW - NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>university</th>\n",
       "      <th>language</th>\n",
       "      <th>paper</th>\n",
       "      <th>email</th>\n",
       "      <th>information</th>\n",
       "      <th>linguistics</th>\n",
       "      <th>address</th>\n",
       "      <th>use</th>\n",
       "      <th>de</th>\n",
       "      <th>...</th>\n",
       "      <th>lifet</th>\n",
       "      <th>erform</th>\n",
       "      <th>promoti</th>\n",
       "      <th>onal</th>\n",
       "      <th>tow</th>\n",
       "      <th>urchase</th>\n",
       "      <th>crespo</th>\n",
       "      <th>tvsrnumbers</th>\n",
       "      <th>promotio</th>\n",
       "      <th>uarantee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2596 rows × 51284 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      number  university  language  paper  email  information  linguistics  \\\n",
       "0          7           0         1      0      0            0            0   \n",
       "1          1           1         0      0      0            0            0   \n",
       "2          0           0         0      0      0            0            0   \n",
       "3         89           6         4      2      1            0            1   \n",
       "4          0           0         0      0      1            0            0   \n",
       "...      ...         ...       ...    ...    ...          ...          ...   \n",
       "2591       6           0         0      1      3            0            0   \n",
       "2592       1           0         0      0      0            0            0   \n",
       "2593      22           0         0      0     10            3            0   \n",
       "2594      26           0         0      0      3            3            0   \n",
       "2595     254           0         0      0      3            3            0   \n",
       "\n",
       "      address  use  de  ...  lifet  erform  promoti  onal  tow  urchase  \\\n",
       "0           0    0   0  ...      0       0        0     0    0        0   \n",
       "1           0    0   0  ...      0       0        0     0    0        0   \n",
       "2           0    0   0  ...      0       0        0     0    0        0   \n",
       "3           1    1   0  ...      0       0        0     0    0        0   \n",
       "4           0    0   0  ...      0       0        0     0    0        0   \n",
       "...       ...  ...  ..  ...    ...     ...      ...   ...  ...      ...   \n",
       "2591        2    0   0  ...      0       0        0     0    0        0   \n",
       "2592        0    0   0  ...      0       0        0     0    0        0   \n",
       "2593        3    4   0  ...      0       0        0     0    0        0   \n",
       "2594        4    0   0  ...      0       0        0     0    0        0   \n",
       "2595        2    1   1  ...      1       1        1     1    1        1   \n",
       "\n",
       "      crespo  tvsrnumbers  promotio  uarantee  \n",
       "0          0            0         0         0  \n",
       "1          0            0         0         0  \n",
       "2          0            0         0         0  \n",
       "3          0            0         0         0  \n",
       "4          0            0         0         0  \n",
       "...      ...          ...       ...       ...  \n",
       "2591       0            0         0         0  \n",
       "2592       0            0         0         0  \n",
       "2593       0            0         0         0  \n",
       "2594       0            0         0         0  \n",
       "2595       1            1         1         1  \n",
       "\n",
       "[2596 rows x 51284 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(word_vectors, columns=vocab)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_X_bag, test_X_bag, train_val_y_bag, test_y_bag = train_test_split(df2, df['label'], test_size=0.2, random_state=42)\n",
    "train_X_bag, val_X_bag, train_y_bag, val_y_bag = train_test_split(train_val_X_bag, train_val_y_bag, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[439   2]\n",
      " [  3  75]]\n",
      "Accuracy:  0.9903660886319846\n",
      "F1 score:  0.9903404667144894\n"
     ]
    }
   ],
   "source": [
    "mnb_1 = MultinomialNB(alpha=1.0) # the default value of alpha\n",
    "mnb_1.fit(train_X_bag,train_y_bag)\n",
    "\n",
    "# Evaluate the performance of the model on the validation set\n",
    "y_pred = mnb_1.predict(val_X_bag)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_pred,val_y_bag))\n",
    "print(\"Accuracy: \", accuracy_score(y_pred,val_y_bag))\n",
    "print(\"F1 score: \", f1_score(y_pred,val_y_bag, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[437   2]\n",
      " [  5  75]]\n",
      "Accuracy:  0.9865125240847784\n",
      "F1 score:  0.9864066267703201\n"
     ]
    }
   ],
   "source": [
    "mnb_05 = MultinomialNB(alpha=0.5) \n",
    "mnb_05.fit(train_X_bag,train_y_bag)\n",
    "\n",
    "# Evaluate the performance of the model on the validation set\n",
    "y_pred = mnb_05.predict(val_X_bag)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_pred,val_y_bag))\n",
    "print(\"Accuracy: \", accuracy_score(y_pred,val_y_bag))\n",
    "print(\"F1 score: \", f1_score(y_pred,val_y_bag, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[438   2]\n",
      " [  4  75]]\n",
      "Accuracy:  0.9884393063583815\n",
      "F1 score:  0.9883783067102637\n"
     ]
    }
   ],
   "source": [
    "mnb_075 = MultinomialNB(alpha=0.75) \n",
    "mnb_075.fit(train_X_bag,train_y_bag)\n",
    "\n",
    "# Evaluate the performance of the model on the validation set\n",
    "y_pred = mnb_075.predict(val_X_bag)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_pred,val_y_bag))\n",
    "print(\"Accuracy: \", accuracy_score(y_pred,val_y_bag))\n",
    "print(\"F1 score: \", f1_score(y_pred,val_y_bag, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[438   2]\n",
      " [  4  75]]\n",
      "Accuracy:  0.9884393063583815\n",
      "F1 score:  0.9883783067102637\n"
     ]
    }
   ],
   "source": [
    "mnb_01 = MultinomialNB(alpha=0.1) \n",
    "mnb_01.fit(train_X_bag,train_y_bag)\n",
    "\n",
    "# Evaluate the performance of the model on the validation set\n",
    "y_pred = mnb_01.predict(val_X_bag)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_pred,val_y_bag))\n",
    "print(\"Accuracy: \", accuracy_score(y_pred,val_y_bag))\n",
    "print(\"F1 score: \", f1_score(y_pred,val_y_bag, average = 'weighted'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the optimal alpha for bag of words vectors is 1.0. Now we can evaluate the final model with the alpha = 1.0 on the test set to obtain an unbiased estimate of its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[427   2]\n",
      " [  2  89]]\n",
      "Accuracy:  0.9923076923076923\n",
      "F1 score:  0.9923076923076923\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the model on the test set\n",
    "y_pred = mnb_1.predict(test_X_bag)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_pred,test_y_bag))\n",
    "print(\"Accuracy: \", accuracy_score(y_pred,test_y_bag))\n",
    "print(\"F1 score: \", f1_score(y_pred,test_y_bag, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gauss = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[441  26]\n",
      " [  1  51]]\n",
      "Accuracy:  0.9479768786127167\n",
      "F1 score:  0.9523024892891224\n"
     ]
    }
   ],
   "source": [
    "gauss.fit(train_X_tfidf,train_y_tfidf)\n",
    "\n",
    "# Evaluate the performance of the model on the validation set\n",
    "y_pred = gauss.predict(val_X_tfidf)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_pred,val_y_tfidf))\n",
    "print(\"Accuracy: \", accuracy_score(y_pred,val_y_tfidf))\n",
    "print(\"F1 score: \", f1_score(y_pred,val_y_tfidf, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "cv_method = RepeatedStratifiedKFold(n_splits=5, \n",
    "                                    n_repeats=3, \n",
    "                                    random_state=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johnny\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3253: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 10 candidates, totalling 150 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=3, n_splits=5, random_state=999),\n",
       "             estimator=GaussianNB(),\n",
       "             param_grid={'var_smoothing': array([1.e+00, 1.e-01, 1.e-02, 1.e-03, 1.e-04, 1.e-05, 1.e-06, 1.e-07,\n",
       "       1.e-08, 1.e-09])},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "params_NB = {'var_smoothing': np.logspace(0,-9, num=10)}\n",
    "\n",
    "gs_NB = GridSearchCV(estimator=gauss, \n",
    "                     param_grid=params_NB, \n",
    "                     cv=cv_method,\n",
    "                     verbose=1, \n",
    "                     scoring='f1')\n",
    "\n",
    "Data_transformed = PowerTransformer().fit_transform(val_X_tfidf)\n",
    "\n",
    "gs_NB.fit(Data_transformed, val_y_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'var_smoothing': 0.001}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_NB.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[367   0]\n",
      " [ 75  77]]\n",
      "Accuracy:  0.8554913294797688\n",
      "F1 score:  0.8385257318428402\n"
     ]
    }
   ],
   "source": [
    "gauss_0001 = GaussianNB(var_smoothing = 0.001)\n",
    "\n",
    "gauss_0001.fit(train_X_tfidf,train_y_tfidf)\n",
    "\n",
    "# Evaluate the performance of the model on the validation set\n",
    "y_pred = gauss_0001.predict(val_X_tfidf)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_pred,val_y_tfidf))\n",
    "print(\"Accuracy: \", accuracy_score(y_pred,val_y_tfidf))\n",
    "print(\"F1 score: \", f1_score(y_pred,val_y_tfidf, average = 'weighted'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turned out that the default var_smoothing for gaussian naive bayes is the best. The \"best\" var_smoothing value found by grid search yielded worse results than the default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[428  30]\n",
      " [  1  61]]\n",
      "Accuracy:  0.9403846153846154\n",
      "F1 score:  0.9450598925431474\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the model on the test set\n",
    "y_pred = gauss.predict(test_X_tfidf)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_pred,test_y_tfidf))\n",
    "print(\"Accuracy: \", accuracy_score(y_pred,test_y_tfidf))\n",
    "print(\"F1 score: \", f1_score(y_pred,test_y_tfidf, average = 'weighted'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same things with bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[442  27]\n",
      " [  0  50]]\n",
      "Accuracy:  0.9479768786127167\n",
      "F1 score:  0.95273597104176\n"
     ]
    }
   ],
   "source": [
    "gauss.fit(train_X_bag,train_y_bag)\n",
    "\n",
    "# Evaluate the performance of the model on the validation set\n",
    "y_pred = gauss.predict(val_X_bag)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_pred,val_y_bag))\n",
    "print(\"Accuracy: \", accuracy_score(y_pred,val_y_bag))\n",
    "print(\"F1 score: \", f1_score(y_pred,val_y_bag, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[210   1]\n",
      " [232  76]]\n",
      "Accuracy:  0.5510597302504817\n",
      "F1 score:  0.4957843892277233\n"
     ]
    }
   ],
   "source": [
    "gauss_0001 = GaussianNB(var_smoothing = 0.001) # Supposedly the best performing var_smoothing\n",
    "\n",
    "gauss_0001.fit(train_X_bag,train_y_bag)\n",
    "\n",
    "# Evaluate the performance of the model on the validation set\n",
    "y_pred = gauss_0001.predict(val_X_bag)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_pred,val_y_bag))\n",
    "print(\"Accuracy: \", accuracy_score(y_pred,val_y_bag))\n",
    "print(\"F1 score: \", f1_score(y_pred,val_y_bag, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[427  23]\n",
      " [  2  68]]\n",
      "Accuracy:  0.9519230769230769\n",
      "F1 score:  0.9544842307121577\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the model on the test set\n",
    "y_pred = gauss.predict(test_X_bag)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_pred,test_y_bag))\n",
    "print(\"Accuracy: \", accuracy_score(y_pred,test_y_bag))\n",
    "print(\"F1 score: \", f1_score(y_pred,test_y_bag, average = 'weighted'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schneider [25] found that the multinomial nb surprisingly performs even better when term frequencies are replaced by Boolean attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "binarizer = Binarizer()\n",
    "df1_bool = binarizer.transform(df1.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_X_tfidf, boo_test_X_tfidf, train_val_y_tfidf, boo_test_y_tfidf = train_test_split(df1_bool, df['label'], test_size=0.2, random_state=42)\n",
    "boo_train_X_tfidf, boo_val_X_tfidf, boo_train_y_tfidf, boo_val_y_tfidf = train_test_split(train_val_X_tfidf, train_val_y_tfidf, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[438  45]\n",
      " [  4  32]]\n",
      "Accuracy:  0.905587668593449\n",
      "F1 score:  0.920623188025101\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "berNB = BernoulliNB(alpha = 1.0) #default alpha\n",
    "berNB.fit(boo_train_X_tfidf,boo_train_y_tfidf)\n",
    "\n",
    "# Evaluate the performance of the model on the validation set\n",
    "y_pred = berNB.predict(boo_val_X_tfidf)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_pred,boo_val_y_tfidf))\n",
    "print(\"Accuracy: \", accuracy_score(y_pred,boo_val_y_tfidf))\n",
    "print(\"F1 score: \", f1_score(y_pred,boo_val_y_tfidf, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[435   1]\n",
      " [  7  76]]\n",
      "Accuracy:  0.9845857418111753\n",
      "F1 score:  0.9843493927782971\n"
     ]
    }
   ],
   "source": [
    "berNB_001 = BernoulliNB(alpha = 0.01) #optimal\n",
    "berNB_001.fit(boo_train_X_tfidf,boo_train_y_tfidf)\n",
    "\n",
    "# Evaluate the performance of the model on the validation set\n",
    "y_pred = berNB_001.predict(boo_val_X_tfidf)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_pred,boo_val_y_tfidf))\n",
    "print(\"Accuracy: \", accuracy_score(y_pred,boo_val_y_tfidf))\n",
    "print(\"F1 score: \", f1_score(y_pred,boo_val_y_tfidf, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[416   1]\n",
      " [ 13  90]]\n",
      "Accuracy:  0.9730769230769231\n",
      "F1 score:  0.9724351944027311\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the model on the test set\n",
    "y_pred = berNB_001.predict(boo_test_X_tfidf)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_pred,boo_test_y_tfidf))\n",
    "print(\"Accuracy: \", accuracy_score(y_pred,boo_test_y_tfidf))\n",
    "print(\"F1 score: \", f1_score(y_pred,boo_test_y_tfidf, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_bool = binarizer.transform(df2.values)\n",
    "train_val_X_bag, boo_test_X_bag, train_val_y_bag, boo_test_y_bag = train_test_split(df2_bool, df['label'], test_size=0.2, random_state=42)\n",
    "boo_train_X_bag, boo_val_X_bag, boo_train_y_bag, boo_val_y_bag = train_test_split(train_val_X_bag, train_val_y_bag, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[435   1]\n",
      " [  7  76]]\n",
      "Accuracy:  0.9845857418111753\n",
      "F1 score:  0.9843493927782971\n"
     ]
    }
   ],
   "source": [
    "berNB_001 = BernoulliNB(alpha = 0.01) #optimal\n",
    "berNB_001.fit(boo_train_X_bag,boo_train_y_bag)\n",
    "\n",
    "# Evaluate the performance of the model on the validation set\n",
    "y_pred = berNB_001.predict(boo_val_X_bag)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_pred,boo_val_y_bag))\n",
    "print(\"Accuracy: \", accuracy_score(y_pred,boo_val_y_bag))\n",
    "print(\"F1 score: \", f1_score(y_pred,boo_val_y_bag, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[417   0]\n",
      " [ 12  91]]\n",
      "Accuracy:  0.9769230769230769\n",
      "F1 score:  0.9763730237737694\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the model on the test set\n",
    "y_pred = berNB_001.predict(boo_test_X_bag)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_pred,boo_test_y_bag))\n",
    "print(\"Accuracy: \", accuracy_score(y_pred,boo_test_y_bag))\n",
    "print(\"F1 score: \", f1_score(y_pred,boo_test_y_bag, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
